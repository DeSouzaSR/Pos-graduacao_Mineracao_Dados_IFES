








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats





path_file = r'../../datasets/segmentation_data.csv'
customer_info = pd.read_csv(path_file)
customer_info.sample(5)


# # Alternativa 1
# import kagglehub
# # Download latest version
# path = kagglehub.dataset_download("dev0914sharma/customer-clustering")
# print("Path to dataset files:", path)
#customer_info = pd.read_csv(path + r'/segmentation data.csv')

# # Alternativa 2
customer_info = pd.read_csv('../../datasets/segmentation_data.csv')
customer_info.sample(5)


customer_info.shape

















customer_info.info()


customer_info.describe()





num_columns = ['ID','Age','Income']
cat_columns = ['Sex','Marital status','Education','Occupation','Settlement size']
customer_info[cat_columns] = customer_info[cat_columns].astype('str')
customer_info.describe(include='object').T

















for numerical in num_columns:
    plt.figure(figsize=(8,4))
    sns.histplot(data=customer_info, x=numerical)








for category in cat_columns:
    plt.figure(figsize=(4,2))
    sns.countplot(data=customer_info, x=category, hue=category)











sns.lmplot(x='Age', y='Income',  line_kws={'color': 'red'},  data=customer_info)





import scipy.stats as stats
print(stats.pearsonr(customer_info['Age'], customer_info['Income']))


sns.lmplot(x='Age', y='Income', hue='Sex', data=customer_info)








cat_columns


for category in cat_columns:
    for numerical in num_columns:
        if numerical != 'ID':
            plt.figure(figsize=(8,4))
            sns.kdeplot(data=customer_info, x=numerical, hue=category) ## Non-normalized version of this plot by switching kdeplot for histplot








cat_aux = cat_columns.copy()
for category1 in cat_columns:
    cat_aux.pop(0);
    for category2 in cat_aux:
        if category1 != category2:
            plt.figure(figsize=(8,4))
            sns.countplot(data=customer_info, x=category1, hue=category2)








def bivariate_scatter(x, y, hue, df):
    plt.figure(figsize=(6,6))
    sns.scatterplot(x=x, y=y, data=df, hue=hue, alpha=0.85)


for cat in cat_columns:
    bivariate_scatter('Age', 'Income', cat, customer_info)











normaltest_result_income = stats.normaltest(customer_info['Income'])[1]
normaltest_result_age    = stats.normaltest(customer_info['Age'])[1]

print(f'O valor p para a hipótese nula de que a renda é distribuída normalmente é {normaltest_result_income}')
print(f'O valor p para a hipótese nula de que a idade é distribuída normalmente é {normaltest_result_age}')











def apply_log(column):
    return np.log(column)

def normality_test(column):
    return stats.normaltest(column)








from sklearn.preprocessing import PowerTransformer


# PowerTransform data
feature = customer_info['Income'].to_numpy().reshape(-1,1)

powtr = PowerTransformer()
feature_transf = powtr.fit_transform(feature)
array_1d = feature_transf.flatten()
feature = pd.Series(data=array_1d, index=list(range(len(array_1d))))

# Log Transform data
log_transformed_income = apply_log(customer_info['Income'])

# Create axis for original data plot (ax1) and transformed data (ax2)
fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4));

# Plot original data & get metrics
customer_info['Income'].plot(kind='hist', ax=ax1)
ax1.title.set_text('Original data')
norm_test1 = normality_test(customer_info['Income'])

# Plot log transformed data & get metrics
log_transformed_income.plot(kind='hist', ax=ax2);
ax2.title.set_text('Log Transformed data')
norm_test2 = normality_test(log_transformed_income)

# Plot power transformed data & get metrics
feature.plot(kind='hist', ax=ax3);
ax3.title.set_text('PowerTransformed data')
norm_test3 = normality_test(feature)

# Create a DataFrame that shows normality test results for each tranformation
norm_results = [norm_test1, norm_test2, norm_test3]
metrics = pd.DataFrame(norm_results, index=['Original data', 'Log transform', 'PowerTransformer'])


normality_test(log_transformed_income)


metrics








# PowerTransform data
feature2 = customer_info['Age'].to_numpy().reshape(-1,1)

powtr = PowerTransformer()
feature_transf = powtr.fit_transform(feature2)
array_1d = feature_transf.flatten()
feature2 = pd.Series(data=array_1d, index=list(range(len(array_1d))))

# Log Transform data
log_transformed_age = apply_log(customer_info['Age'])

# Create axis for original data plot (ax1) and transformed data (ax2)
fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4));

# Plot original data & get metrics
customer_info['Age'].plot(kind='hist', ax=ax1)
ax1.title.set_text('Original data')
norm_test1 = normality_test(customer_info['Age'])

# Plot log transformed data & get metrics
log_transformed_age.plot(kind='hist', ax=ax2);
ax2.title.set_text('Log Transformed data')
norm_test2 = normality_test(log_transformed_age)

# Plot power transformed data & get metrics
feature2.plot(kind='hist', ax=ax3);
ax3.title.set_text('PowerTransformed data')
norm_test3 = normality_test(feature2)

# Create a DataFrame that shows normality test results for each tranformation
norm_results = [norm_test1, norm_test2, norm_test3]
metrics = pd.DataFrame(norm_results, index=['Original data', 'Log transform', 'PowerTransformer'])


metrics








customer_info['transf_income'] = feature
customer_info['transf_age']    = log_transformed_age


customer_transformed = customer_info.drop(['Income', 'Age', 'ID'], axis=1)
customer_transformed





from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler()
X = scaler.fit_transform(customer_transformed)


X[0]





from sklearn.cluster import KMeans


clusters_range=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]
inertias=[]

for c in clusters_range:
    kmeans=KMeans(n_clusters=c, n_init=10, random_state=42).fit(X)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(7,7))
plt.plot(clusters_range,inertias, marker='o')








from sklearn.metrics import silhouette_samples, silhouette_score

clusters_range=range(2, 20)
random_range  =range(0, 20)
results=[]

for c in clusters_range:
    for r in random_range:
        clusterer=KMeans(n_clusters=c, n_init=10, random_state=r)
        cluster_labels=clusterer.fit_predict(X)
        silhouette_avg=silhouette_score(X, cluster_labels)
        results.append([c,r,silhouette_avg])

result  =pd.DataFrame(results, columns=["n_clusters","seed","silhouette_score"])
pivot_km=pd.pivot_table(result, index="n_clusters", columns="seed",values="silhouette_score")

plt.figure(figsize=(15, 6))
sns.heatmap(pivot_km, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm.rocket_r)
plt.tight_layout()














from sklearn.decomposition import PCA


pca = PCA(n_components=3, random_state=42)
X_pca = pca.fit_transform(X)





X_pca_df = pd.DataFrame(data=X_pca, columns=['X1', 'X2', 'X3'])





kmeans=KMeans(n_clusters=6, n_init=10, random_state=0).fit(X)





labels = kmeans.labels_
X_pca_df['Labels'] = labels





from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import cufflinks as cf
init_notebook_mode(connected=True)
cf.go_offline()


X_pca_df.head()


# @title X1 vs X2

import matplotlib.pyplot as plt
plt.scatter(X_pca_df['X1'], X_pca_df['X2'], c=X_pca_df['Labels'])
plt.xlabel('X1')
_ = plt.ylabel('X2')





# @title X1 vs X2 colored by label

sns.scatterplot(data=X_pca_df, x="X1", y="X2", hue="Labels")





X_pca_df['Labels'] = X_pca_df['Labels'].astype(str)





import plotly.express as px

fig = px.scatter_3d(X_pca_df, x='X1', y='X2', z='X3',
              color=X_pca_df['Labels'])
fig.show()








results_df = customer_info.drop(['ID', 'transf_income', 'transf_age'], axis=1)
results_df['Labels'] = kmeans.labels_
results_df = results_df.astype({'Sex':'int32', 'Marital status':'int32', 'Education':'int32', 'Occupation':'int32', 'Settlement size':'int32'})
results_df.info()








summary = {}

for index in range(6):
    summary[index] = results_df[results_df['Labels'] == index].describe().T  # .describe method provides general statistics about the data











summary[0]


results_df[results_df['Labels'] == 0].hist(figsize=(10,10));








summary[3]


results_df[results_df['Labels'] == 3].hist(figsize=(8,8));








summary[1]


results_df[results_df['Labels'] == 1].hist(figsize=(8,9));








summary[2]


results_df[results_df['Labels'] == 2].hist(figsize=(8,8));
plt.savefig('cluster4_results')





from scipy.stats import ttest_ind
import numpy as np

age_cluster_0 = results_df[results_df['Labels'] == 0]['Age']
age_cluster_1 = results_df[results_df['Labels'] == 1]['Age']
age_cluster_2 = results_df[results_df['Labels'] == 2]['Age']
age_cluster_3 = results_df[results_df['Labels'] == 3]['Age']

first_clusters  = [1, 2]
second_clusters = [0, 3]
real_clusters = [age_cluster_0, age_cluster_1, age_cluster_2, age_cluster_3]

for num_clust_1 in first_clusters:
    for num_clust_2 in second_clusters:
        ttest,pval = ttest_ind(real_clusters[num_clust_1], real_clusters[num_clust_2])
        print(f'p-value of {num_clust_1} vs {num_clust_2} is:',pval)
        if pval <0.05:
            print("we reject null hypothesis")
        else:
            print("we accept null hypothesis")






summary[4]


results_df[results_df['Labels'] == 4].hist(figsize=(8,8));








summary[5]


results_df[results_df['Labels'] == 5].hist(figsize=(8,8));











centroids = kmeans.cluster_centers_
pd.DataFrame(centroids, columns = results_df.columns[:7], index = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5'])








from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import graphviz


from sklearn.tree import DecisionTreeClassifier
from sklearn import tree


clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 5)


X_clusters = results_df.drop('Labels', axis=1)
y_clusters = results_df['Labels']

clf.fit(X_clusters, y_clusters)





from sklearn.metrics import classification_report, confusion_matrix


predictions = clf.predict(X_clusters)
print(classification_report(y_clusters, predictions))










# DOT data
dot_data = tree.export_graphviz(clf, out_file=None,
                                feature_names=results_df.columns[:7],
                                class_names=['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'],
                                filled=True)

# Draw graph
graph = graphviz.Source(dot_data, format="png")
graph
#plt.savefig('DecisionTree.png')














# Features
features = ['Sex', 'Marital status', 'Age', 'Education', 'Income', 'Occupation', 'Settlement size']

# Obtendo a importância de cada variável
importances = clf.feature_importances_

# Criando um DataFrame para exibir os valores de forma organizada
feature_importance_df = pd.DataFrame({'Variável': features, 'Importância': importances})

# Ordenando da variável mais importante para a menos importante
feature_importance_df = feature_importance_df.sort_values(by='Importância', ascending=False)

# Exibindo a importância de cada variável
print(feature_importance_df)



from sklearn.tree import export_text


# Exportando a árvore em formato de texto
tree_rules = export_text(clf, feature_names=features)

# Exibindo as regras
print(tree_rules)









